# ./justfile

# Helper to validate arch
_validate_arch arch:
    #!/usr/bin/env bash
    case "{{arch}}" in
        x86_64|arm64)
            ;;
        *)
            echo "Error: Invalid architecture '{{arch}}'. Must be 'x86_64' or 'arm64'."
            exit 1
            ;;
    esac

# Full pipeline from build to AMI registration
aws-all bucket-name arch:
    #!/usr/bin/env bash
    set -e
    just _validate_arch {{arch}}
    just aws-build {{arch}}
    just aws-upload {{bucket-name}} {{arch}}

aws-build arch:
    #!/usr/bin/env bash
    set -e
    set -x
    just _validate_arch {{arch}}

    NIX_ARCH=""
    if [ "{{arch}}" == "x86_64" ]; then
        NIX_ARCH="x86_64-linux"
    else
        NIX_ARCH="aarch64-linux"
    fi

    echo "Building AWS AMI for ${NIX_ARCH}..."
    nix build ".#nixosConfigurations.aws-base-${NIX_ARCH}.config.formats.amazon"
    
    echo "Build complete! Image at: ./result/"
    ls -lh ./result/

aws-upload bucket-name arch region="eu-west-1":
    #!/usr/bin/env bash
    set -e
    set -x
    just _validate_arch {{arch}}

    BUCKET_NAME="{{bucket-name}}"
    REGION="{{region}}"
    IMAGE_NAME="nixos-{{arch}}-$(date +%Y%m%d-%H%M%S)"
    
    VHD_FILE=$(find -L ./result/ -name "*.vhd" -type f | head -n1)
    
    echo "Uploading ${VHD_FILE} to s3://${BUCKET_NAME}/${IMAGE_NAME}.vhd"
    aws s3 cp ${VHD_FILE} s3://${BUCKET_NAME}/${IMAGE_NAME}.vhd --region ${REGION}
    
    cat > /tmp/containers.json <<EOF
    {
      "Description": "NixOS Custom AMI (${IMAGE_NAME})",
      "Format": "vhd",
      "UserBucket": {
        "S3Bucket": "${BUCKET_NAME}",
        "S3Key": "${IMAGE_NAME}.vhd"
      }
    }
    EOF
    
    IMPORT_TASK_ID=$(aws ec2 import-snapshot \
      --region ${REGION} \
      --description "NixOS Custom Image ${IMAGE_NAME}" \
      --disk-container file:///tmp/containers.json \
      --query 'ImportTaskId' \
      --output text)
    
    echo "Import Task ID: ${IMPORT_TASK_ID}"
    echo "Monitoring import... this will take several minutes."

    aws ec2 wait snapshot-imported --import-task-ids ${IMPORT_TASK_ID} --region ${REGION}
    echo "Snapshot import complete!"

    # Now, register the AMI
    just aws-register ${IMPORT_TASK_ID} {{arch}} ${REGION}

aws-register import-task-id arch region="eu-west-1":
    #!/usr/bin/env bash
    set -e
    set -x
    just _validate_arch {{arch}}

    SNAPSHOT_ID=$(aws ec2 describe-import-snapshot-tasks \
      --region {{region}} \
      --import-task-ids {{import-task-id}} \
      --query 'ImportSnapshotTasks[0].SnapshotTaskDetail.SnapshotId' \
      --output text)
    
    AMI_NAME="nixos-{{arch}}-$(date +%Y%m%d-%H%M%S)"
    
    AMI_ID=$(aws ec2 register-image \
      --region {{region}} \
      --name ${AMI_NAME} \
      --description "Custom NixOS {{arch}} AMI with K3s" \
      --architecture {{arch}} \
      --root-device-name /dev/xvda \
      --block-device-mappings "[{\"DeviceName\":\"/dev/xvda\",\"Ebs\":{\"SnapshotId\":\"${SNAPSHOT_ID}\",\"VolumeType\":\"gp3\",\"DeleteOnTermination\":true}}]" \
      --virtualization-type hvm \
      --ena-support \
      --query 'ImageId' \
      --output text)
    
    echo ""
    echo "✓ AMI Created Successfully!"
    echo "  AMI ID: ${AMI_ID}"
    echo "  Region: {{region}}"
    echo "  Name:   ${AMI_NAME}"
    echo "  Arch:   {{arch}}"

localstack-img:
    #!/usr/bin/env bash
    set -e
    set -x

    # --- Configuration ---
    NIX_CONFIG="localstack-base"
    AMI_NAME="nixos-base-image"
    AMI_ID="ami-nixosbase01"
    # This is the temporary tag we will import as.
    BASE_IMAGE_TAG="nixos-localstack-base:latest"
    # This is the final tag LocalStack recognizes.
    FINAL_TAG="localstack-ec2/${AMI_NAME}:${AMI_ID}"

    RESULT_PATH="/tmp/nixos-docker-tarball/"

    # 1. Build the system tarball.
    #    The `docker-image.nix` module configures this specific output.
    echo "Building NixOS system tarball for Docker..."
    nix build ".#nixosConfigurations.${NIX_CONFIG}.config.formats.docker" --impure -o "${RESULT_PATH}"

    # 2. Find the output tarball. There should be only one.
    TARBALL_PATH=$(find "${RESULT_PATH}" -name '*.tar.xz')
    if [ -z "$TARBALL_PATH" ]; then
        echo "Error: Could not find built .tar.xz in ${RESULT_PATH}/tarball"
        exit 1
    fi
    echo "Found tarball at: ${TARBALL_PATH}"

    # 3. Import the tarball into Docker.
    #    'docker import' reads a filesystem tarball, not a 'docker save' tarball.
    echo "Importing tarball into Docker as ${BASE_IMAGE_TAG}..."
    docker import "${TARBALL_PATH}" "${BASE_IMAGE_TAG}"

    # 4. Tag the imported image with the special LocalStack format.
    echo "Tagging image as ${FINAL_TAG} for LocalStack..."
    docker tag "${BASE_IMAGE_TAG}" "${FINAL_TAG}"

    # 5. Clean up the result symlink.
    rm "${RESULT_PATH}"

    echo ""
    echo "✅ Image ready for LocalStack!"
    echo "---"
    echo "To verify, run:"
    echo "  awslocal ec2 describe-images --filters Name=image-id,Values=${AMI_ID}"
    echo ""
    echo "To launch an instance (which will run systemd via /init), run:"
    echo "  awslocal ec2 run-instances --image-id ${AMI_ID} --instance-type t2.micro"

img $site $user: 
    #!/usr/bin/env bash
    set -e
    set -x
    export SITE={{site}} USER={{user}}
    export FILE_IMAGE_BASEURL=http://public.$SITE.grid5000.fr/~$USER
    nix build .#nixosConfigurations.g5k-base.config.formats.g5k-image --impure -o /tmp/result
   
    PUBLIC_DIR="public/"
    REMOTE_USER="$USER@$SITE.grid5000.fr"

    JSON_FILE="/tmp/result/g5k-image-info.json"

    echo "Found JSON file at: $JSON_FILE"

    # Parse the JSON file to get paths
    IMAGE_PATH=$(jq -r '.image' "$JSON_FILE")
    KAENV_PATH=$(jq -r '.kaenv' "$JSON_FILE")
    IMAGE_FILENAME=$(basename "$IMAGE_PATH")
    KAENV_FULL_FILENAME=$(basename "$KAENV_PATH")
    # Extract the part after the last dash if its a Nix store path
    if [[ "$KAENV_FULL_FILENAME" =~ ^[a-z0-9]+-(.+)$ ]]; then
        KAENV_FILENAME="${BASH_REMATCH[1]}"
    else
        KAENV_FILENAME="$KAENV_FULL_FILENAME"
    fi


    echo "Syncing $IMAGE_FILENAME and $KAENV_FILENAME to $SITE"
    ssh "$REMOTE_USER" "mkdir -p $PUBLIC_DIR"
    rsync -avz --progress "$IMAGE_PATH" "$REMOTE_USER:$PUBLIC_DIR/$IMAGE_FILENAME"

    echo "Uploading kaenv file..."
    # Upload with the clean filename
    rsync -avz "$KAENV_PATH" "$REMOTE_USER:$PUBLIC_DIR/$KAENV_FILENAME"

    
    echo "Registering environment with kaenv3..."
    ssh "$REMOTE_USER" "kaenv3 -d nixos-x86_64-linux -y" || true
    ssh "$REMOTE_USER" "kaenv3 -a $PUBLIC_DIR/$KAENV_FILENAME"
    
host *args:
    #!/usr/bin/env bash

    # Function to display usage information
    usage() {
        echo "Usage: $0 <command> [options]"
        echo ""
        echo "Commands:"
        echo "  server <hostname> [site]       - Generate server configuration"
        echo "  worker <hostname> <server> [site] - Generate worker configuration"
        echo "  broker <hostname> <server> [site] - Generate broker configuration"
        echo ""
        echo "Examples:"
        echo "  $0 server grappe-10 nancy"
        echo "  $0 worker grappe-11 grappe-10 nancy"
        echo "  $0 broker grappe-12 grappe-10 nancy"
        exit 1
    }

    # Check for minimum arguments
    if [ $# -lt 2 ]; then
        usage
    fi

    command="$1"
    hostname="$2"
    output_dir="nixos-configurations"
    mkdir -p "$output_dir"

    # Generate configuration based on command
    case "$command" in
        server)
            site="${3:-nancy}"
            output_file="${output_dir}/${hostname}.nix"
            
            cat > "$output_file" << EOF
    {
      inputs,
      lib,
      pkgs,
      config,
      ezModules,
      ...
    }: let
      name = "${hostname}";
      site = "${site}";
    in {
      imports = [
        ezModules.g5k
        ezModules.k3s-overlay
      ];
      networking.hostName = name;
      services.k3s = {
        role = "server";
        extraFlags = lib.concatStringsSep " " [
          "--disable traefik"
          "--tls-san \${name}.\${site}.grid5000.fr"
          "--tls-san \${name}"
          # "--docker"
          "--kubelet-arg=eviction-hard=nodefs.available<1%,imagefs.available<1%,nodefs.inodesFree<1%"
        ];
      };
    }
    EOF
            echo "Server configuration created at: $output_file"
            ;;
            
        worker)
            if [ $# -lt 3 ]; then
                echo "Error: Server hostname is required for worker configuration"
                usage
            fi
            server="$3"
            site="${4:-nancy}"
            output_file="${output_dir}/${hostname}.nix"
            
            cat > "$output_file" << EOF
    {
      inputs,
      lib,
      pkgs,
      config,
      ezModules,
      ...
    }: let
      name = "${hostname}";
      serverAddr = "https://${server}.${site}.grid5000.fr:6443";
    in {
      imports = [
        ezModules.g5k
        ezModules.k3s-overlay
      ];
      networking.hostName = name;
      services.k3s = {
        role = "agent";
        serverAddr = serverAddr;
        extraFlags = lib.concatStringsSep " " [
          "--node-label=node.kubernetes.io/worker=true"
          "--node-name=\${name}"
          "--kubelet-arg=eviction-hard=nodefs.available<1%,imagefs.available<1%,nodefs.inodesFree<1%"
        ];
      };
    }
    EOF
            echo "Worker configuration created at: $output_file"
            ;;
            
        broker)
            if [ $# -lt 3 ]; then
                echo "Error: Server hostname is required for broker configuration"
                usage
            fi
            server="$3"
            site="${4:-nancy}"
            output_file="${output_dir}/${hostname}.nix"
            
            cat > "$output_file" << EOF
    {
      inputs,
      lib,
      pkgs,
      config,
      ezModules,
      ...
    }: let
      name = "${hostname}";
      serverAddr = "https://${server}.${site}.grid5000.fr:6443";
    in {
      imports = [
        ezModules.g5k
        ezModules.k3s-overlay
      ];
      networking.hostName = name;
      services.k3s = {
        role = "agent";
        serverAddr = serverAddr;
        extraFlags = lib.concatStringsSep " " [
          "--node-label=node.kubernetes.io/broker=true"
          "--node-name=\${name}"
          "--kubelet-arg=eviction-hard=nodefs.available<1%,imagefs.available<1%,nodefs.inodesFree<1%"
        ];
      };
    }
    EOF
            echo "Broker configuration created at: $output_file"
            ;;
            
        *)
            echo "Error: Unknown command '$command'"
            usage
            ;;
    esac

# Setup AWS vmimport role (one-time setup per AWS account)
aws-setup-vmimport bucket-name region="eu-west-1":
    #!/usr/bin/env bash
    set -e
    set -x
    
    BUCKET_NAME="{{bucket-name}}"
    REGION="{{region}}"
    
    echo "Setting up vmimport IAM role for AWS VM Import/Export..."
    
    # Create trust policy
    cat > /tmp/trust-policy.json <<EOF
    {
       "Version": "2012-10-17",
       "Statement": [
          {
             "Effect": "Allow",
             "Principal": { "Service": "vmie.amazonaws.com" },
             "Action": "sts:AssumeRole",
             "Condition": {
                "StringEquals":{
                   "sts:Externalid": "vmimport"
                }
             }
          }
       ]
    }
    EOF
    
    # Create the IAM role
    echo "Creating vmimport role..."
    if aws iam get-role --role-name vmimport 2>/dev/null; then
        echo "Role vmimport already exists, skipping creation..."
    else
        aws iam create-role \
            --role-name vmimport \
            --assume-role-policy-document file:///tmp/trust-policy.json
        echo "✓ Role created"
    fi
    
    # Create role policy
    cat > /tmp/role-policy.json <<EOF
    {
       "Version":"2012-10-17",
       "Statement":[
          {
             "Effect": "Allow",
             "Action": [
                "s3:GetBucketLocation",
                "s3:GetObject",
                "s3:ListBucket" 
             ],
             "Resource": [
                "arn:aws:s3:::${BUCKET_NAME}",
                "arn:aws:s3:::${BUCKET_NAME}/*"
             ]
          },
          {
             "Effect": "Allow",
             "Action": [
                "ec2:ModifySnapshotAttribute",
                "ec2:CopySnapshot",
                "ec2:RegisterImage",
                "ec2:Describe*"
             ],
             "Resource": "*"
          }
       ]
    }
    EOF
    
    # Attach the policy to the role
    echo "Attaching policy to vmimport role..."
    aws iam put-role-policy \
        --role-name vmimport \
        --policy-name vmimport \
        --policy-document file:///tmp/role-policy.json
    echo "✓ Policy attached"
    
    # Verify the role
    echo "Verifying role..."
    aws iam get-role --role-name vmimport --query 'Role.RoleName' --output text
    
    # Clean up temp files
    rm /tmp/trust-policy.json /tmp/role-policy.json
    
    echo ""
    echo "✓ vmimport role setup complete!"
    echo "  You can now run: just aws-upload {{bucket-name}} <arch>"


